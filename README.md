ğŸ“Œ PySpark DataFrame Operations â€“ Learning Notebook
ğŸ“– Overview

This repository contains my hands-on PySpark learning notebook, where I explored and practiced core PySpark DataFrame operations using Databricks.
The notebook covers data reading, schema handling, transformations, filtering, aggregations, string & date functions, and more.

This project helped me build a strong foundation in big data processing using Apache Spark (PySpark).

ğŸ› ï¸ Technologies Used

Apache Spark (PySpark)

Databricks Notebook

Python

CSV & JSON Data Sources

ğŸ“‚ Data Sources

JSON File

Used to understand JSON reading and schema inference

CSV File (BigMart Sales Dataset)

Used for most DataFrame operations and transformations

ğŸš€ Concepts Covered
ğŸ”¹ Data Reading

Reading JSON files

Reading CSV files

Using inferSchema

Handling headers

Working with Databricks file system (dbutils.fs.ls)

ğŸ”¹ Schema Management

Auto-inferred schema

DDL Schema definition

StructType & StructField schema

Schema comparison and validation

ğŸ”¹ DataFrame Operations

select() with columns

Column alias using alias()

filter() with:

AND (&)

OR (|)

isin()

isNull()

withColumn() for:

Creating new columns

Arithmetic operations

Constants using lit()

withColumnRenamed()

ğŸ”¹ String Functions

regexp_replace() for data standardization

initcap()

upper()

lower()

ğŸ”¹ Type Casting

Converting column data types using cast()

Handling numeric operations safely

ğŸ”¹ Sorting & Limiting

orderBy() / sort()

Ascending & descending sort

Sorting with multiple columns

limit() function

ğŸ”¹ Handling Duplicates

drop()

dropDuplicates()

distinct()

ğŸ”¹ Union Operations

union()

unionByName()

Understanding schema alignment issues

ğŸ”¹ Date Functions

current_date()

date_add()

date_sub()

datediff()

date_format()

ğŸ”¹ Null Handling

dropna()

fillna()

Dropping nulls using subsets

ğŸ”¹ Split & Indexing

split() function

Extracting values using getItem()

ğŸ”¹ Group By & Aggregations

groupBy()

Aggregation functions:

sum()

avg()

Grouping with multiple columns

Aliasing aggregated columns

ğŸ¯ Learning Outcome

After completing this notebook, I gained:

Strong understanding of PySpark DataFrame API

Ability to clean, transform, and analyze large datasets

Practical experience with real-world data scenarios

Confidence to use PySpark for data engineering workflows

ğŸ“Œ How to Use

Clone the repository

Open the notebook in Databricks

Update file paths if needed

Run cells sequentially

ğŸ“ Author

Niruta Kadel
Data Enthusiast | PySpark Learner
