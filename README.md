# ğŸ“Š PySpark DataFrame Operations â€“ Advanced Tutorial Project

This repository contains a **complete endâ€‘toâ€‘end PySpark tutorial project** built by combining:

* ğŸ“˜ **Advanced PySpark notebook / PDF concepts**
* ğŸ““ **Handsâ€‘on Databricks notebook implementation**
* ğŸ§ª **Real CSV and JSON datasets**

The project demonstrates **beginner â†’ advanced PySpark DataFrame operations**, making it ideal for **students, interns, and aspiring Data Engineers / Data Scientists**.

---

## ğŸ“Œ Project Overview

The goal of this project is to provide **practical exposure to PySpark** using realâ€‘world style data. It covers the full lifecycle of working with Spark DataFrames â€” from **data ingestion** to **advanced analytics using window functions and UDFs**.

All examples are executed in a **Databricks / Spark environment** using PySpark APIs.

---

## ğŸ› ï¸ Technologies Used

* **Apache Spark (PySpark)**
* **Databricks Notebook**
* **Python**

---

## ğŸ“‚ Data Sources

The following datasets are used throughout the project:

* **CSV File**: `BigMart Sales.csv`
* **JSON File**: `drivers.json`

### Example Databricks Volume Paths

```
/Volumes/workspace/pysparkcsv/csvfile/
/Volumes/workspace/pysparkcsv/sparkjson/
```

---

## ğŸ“– Topics Covered

### 1ï¸âƒ£ Data Reading

* Reading CSV files using `spark.read.format("csv")`
* Reading JSON files using `spark.read.format("json")`
* Options used:

  * `inferSchema`
  * `header`
  * `multiline`

---

### 2ï¸âƒ£ Schema Definition

* Infer Schema
* DDLâ€‘based schema
* `StructType` & `StructField` schema definition

---

### 3ï¸âƒ£ Data Exploration

* `display()` for visualization (Databricks)
* `printSchema()`
* `select()` with column aliasing
* `collect()` to bring data to driver as Row objects

---

### 4ï¸âƒ£ Filtering Data

* Filtering using column conditions
* Multiple conditions with `AND` / `OR`
* `isin()` usage
* `isNull()` and `isNotNull()`

---

### 5ï¸âƒ£ Column Operations

* `withColumn()`
* `withColumnRenamed()`
* Creating constant columns using `lit()`
* Arithmetic column calculations

---

### 6ï¸âƒ£ String Functions

* `initcap()`
* `upper()`
* `lower()`
* `regexp_replace()` for standardization

---

### 7ï¸âƒ£ Type Casting

* Converting column data types using `cast()`

---

### 8ï¸âƒ£ Sorting & Limiting

* `orderBy()` / `sort()`
* Sorting with multiple columns
* `limit()`

---

### 9ï¸âƒ£ Handling Null Values

* `dropna()`
* `fillna()`
* Subsetâ€‘based null handling

---

### ğŸ”Ÿ Duplicate Handling

* `dropDuplicates()`
* `distinct()`

---

### 1ï¸âƒ£1ï¸âƒ£ Union Operations

* `union()`
* `unionByName()`

---

### 1ï¸âƒ£2ï¸âƒ£ Aggregations & Grouping

* `groupBy()` with:

  * `sum()`
  * `avg()`
* Multiâ€‘column grouping
* Aliasing aggregated columns

---

### 1ï¸âƒ£3ï¸âƒ£ Pivot Operations

* Pivoting data using `pivot()`
* Aggregation after pivot (e.g., average Item_MRP by Outlet_Size)

---

### 1ï¸âƒ£4ï¸âƒ£ Conditional Logic (WHEN / OTHERWISE)

* Creating conditional columns using `when()` and `otherwise()`
* Nested conditions
* Realâ€‘world example: veg / nonâ€‘veg classification and priceâ€‘based categories

---

### 1ï¸âƒ£5ï¸âƒ£ Joins in PySpark

Implemented join types:

* Inner Join
* Left Join
* Right Join
* Full Outer Join (conceptual)
* Left Semi Join (conceptual)
* Left Anti Join
* Cross Join (conceptual)

Practical examples using **employee and department datasets**.

---

### 1ï¸âƒ£6ï¸âƒ£ Window Functions

Window functions used without collapsing rows:

* `row_number()`
* `rank()`
* `dense_rank()`
* Cumulative Sum (`sum() over window`)

Window specifications include:

* `partitionBy()`
* `orderBy()`
* `rowsBetween()`

---

### 1ï¸âƒ£7ï¸âƒ£ User Defined Functions (UDF)

* Creating Python functions
* Registering them as PySpark UDFs
* Applying UDFs to DataFrame columns

Example: Squaring numeric column values using UDF

---

## ğŸš€ How to Run the Project

1. Upload `AdvancePyspark.ipynb` / `.py` to **Databricks** or Spark environment
2. Ensure datasets exist at configured paths
3. Execute cells sequentially to understand each operation

---

## ğŸ¯ Learning Outcomes

After completing this project, you will be able to:

* Perform **advanced PySpark DataFrame operations**
* Work with schemas, joins, and window functions
* Handle realâ€‘world data quality issues
* Apply conditional logic and aggregations
* Build strong foundations for **Big Data & Data Engineering roles**

---

## ğŸ“Œ Future Enhancements

* Spark SQL queries
* Performance optimization & caching
* Broadcast joins
* PySpark unit testing

---

## ğŸ“˜ Reference Material

This project is built by combining **Advanced PySpark learning material (PDF)** and **handsâ€‘on Databricks notebook implementations**, focusing on industryâ€‘relevant DataFrame processing techniques.

---

## ğŸ“„ License

This project is intended for **learning and educational purposes only**.

---

## â­ Support

If you find this repository helpful:

* â­ Star the repo
* ğŸ´ Fork it
* ğŸ“¢ Share with other learners
